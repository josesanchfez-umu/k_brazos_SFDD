{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio de Ascenso de Gradientes con una distribución Benoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\jasf7\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip', 'c:\\\\Users\\\\jasf7\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs', 'c:\\\\Users\\\\jasf7\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib', 'c:\\\\Users\\\\jasf7\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312', '', 'c:\\\\Users\\\\jasf7\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jasf7\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jasf7\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jasf7\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\Pythonwin', './src/', './src/', './src/', './src/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Añadir los directorios fuentes al path de Python\n",
    "sys.path.append('./src/')\n",
    "\n",
    "\n",
    "# Verificar que se han añadido correctamente\n",
    "print(sys.path)\n",
    "\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from algorithms import Algorithm, softmax, gradient_bandit\n",
    "from arms.armbernoulli import ArmBernoulli\n",
    "from arms import Bandit\n",
    "from plotting import plot_average_rewards, plot_optimal_selections, plot_arm_statistics, plot_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def run_experiment(bandit: Bandit, algorithms: List[Algorithm], steps: int, runs: int):\n",
    "\n",
    "    optimal_arm = bandit.optimal_arm  # Necesario para calcular el porcentaje de selecciones óptimas.\n",
    "\n",
    "    optimal_reward = bandit.get_expected_value(optimal_arm) # Recompensa esperada del brazo óptimo.\n",
    "\n",
    "    rewards = np.zeros((len(algorithms), steps)) # Matriz para almacenar las recompensas promedio.\n",
    "\n",
    "    optimal_selections = np.zeros((len(algorithms), steps))  # Matriz para almacenar el porcentaje de selecciones óptimas.\n",
    "\n",
    "    regret_accumulated = np.zeros((len(algorithms), steps))  # Matriz del rechazo acumulado\n",
    "\n",
    "    arm_stats = [(algo, {arm: [0, 0] for arm in range(len(bandit.arms))}) for algo in algorithms] # TODO: Lista para almacenar estadísticas por cada algoritmo\n",
    "\n",
    "    np.random.seed(seed)  # Asegurar reproducibilidad de resultados.\n",
    "\n",
    "    for run in range(runs):\n",
    "        current_bandit = Bandit(arms=bandit.arms)\n",
    "\n",
    "        for algo in algorithms:\n",
    "            algo.reset() # Reiniciar los valores de los algoritmos.\n",
    "\n",
    "        total_rewards_per_algo = np.zeros(len(algorithms)) # Acumulador de recompensas por algoritmo. Necesario para calcular el promedio.\n",
    "\n",
    "        cumulative_regret = np.zeros(len(algorithms))   # Acumulador de rechazo acumulado por algoritmo.\n",
    "\n",
    "        for step in range(steps):\n",
    "            for idx, algo in enumerate(algorithms):\n",
    "                chosen_arm = algo.select_arm() # Seleccionar un brazo según la política del algoritmo.\n",
    "                reward = current_bandit.pull_arm(chosen_arm) # Obtener la recompensa del brazo seleccionado.\n",
    "                algo.update(chosen_arm, reward) # Actualizar el valor estimado del brazo seleccionado.\n",
    "\n",
    "                rewards[idx, step] += reward # Acumular la recompensa obtenida en la matriz rewards para el algoritmo idx en el paso step.\n",
    "                total_rewards_per_algo[idx] += reward # Acumular la recompensa obtenida en total_rewards_per_algo para el algoritmo idx.\n",
    "\n",
    "                # TODO: (arm_statistics) Buscar el conjunto de estadísticas correspondiente a este algoritmo\n",
    "                _, stats = arm_stats[idx]\n",
    "                # Registrar selecciones y recompensas en arm_stats\n",
    "                stats[chosen_arm][0] += 1  # Contador de selecciones\n",
    "                stats[chosen_arm][1] += reward  # Suma de recompensas\n",
    "\n",
    "                #TODO: (optimal_selections) modificar optimal_selections cuando el brazo elegido se corresponda con el brazo óptimo optimal_arm\n",
    "                if chosen_arm == optimal_arm:\n",
    "                    optimal_selections[idx, step] += 1\n",
    "\n",
    "                # Cálculo del rechazo\n",
    "                regret = optimal_reward - reward  \n",
    "                cumulative_regret[idx] += regret  # Acumular regret total\n",
    "                regret_accumulated[idx, step] += cumulative_regret[idx] \n",
    "\n",
    "\n",
    "    rewards /= runs\n",
    "\n",
    "    # TODO: (optimal_selections) calcular el porcentaje de selecciones óptimas y almacenar en optimal_selections\n",
    "    optimal_selections = (optimal_selections / runs) * 100  # Convertir a porcentaje\n",
    "\n",
    "    regret_accumulated /= runs  # Calcular el rechazo acumulado promedio\n",
    "\n",
    "    # TODO: (arm_statistics) Calcular la recompensa promedio para cada brazo\n",
    "    for _, stats in arm_stats:\n",
    "        for arm in stats:\n",
    "            selections, total_reward = stats[arm]\n",
    "            stats[arm] = (selections, total_reward / selections if selections > 0 else 0)\n",
    "\n",
    "    return rewards, optimal_selections, arm_stats, regret_accumulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ArmBernoulli' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m  \u001b[38;5;66;03m# Número de ejecuciones\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Creación del bandit\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m bandit \u001b[38;5;241m=\u001b[39m Bandit(arms\u001b[38;5;241m=\u001b[39m\u001b[43mArmBernoulli\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate_arms(k)) \u001b[38;5;66;03m# Generar un bandido con k brazos de distribución normal\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(bandit)\n\u001b[0;32m     13\u001b[0m optimal_arm \u001b[38;5;241m=\u001b[39m bandit\u001b[38;5;241m.\u001b[39moptimal_arm\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ArmBernoulli' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parámetros del experimento\n",
    "seed = 42\n",
    "np.random.seed(seed)  # Fijar la semilla para reproducibilidad\n",
    "\n",
    "k = 10  # Número de brazos\n",
    "steps = 1000  # Número de pasos que se ejecutarán cada algoritmo\n",
    "runs = 500  # Número de ejecuciones\n",
    "\n",
    "# Creación del bandit\n",
    "bandit = Bandit(arms=ArmBernoulli.generate_arms(k)) # Generar un bandido con k brazos de distribución normal\n",
    "print(bandit)\n",
    "\n",
    "optimal_arm = bandit.optimal_arm\n",
    "print(f\"Optimal arm: {optimal_arm + 1} with expected reward={bandit.get_expected_value(optimal_arm)}\")\n",
    "\n",
    "# Definir los algoritmos a comparar. En este caso son 3 algoritmos epsilon-greedy con diferentes valores de epsilon.\n",
    "algorithms = [softmax(k=k, epsilon=0, temperature=0.1), softmax(k=k, epsilon=0, temperature=0.1), softmax(k=k, epsilon=0, temperature=0.1)]\n",
    "\n",
    "# Ejecutar el experimento y obtener las recompensas promedio y promedio de las selecciones óptimas\n",
    "rewards, optimal_selections, arm_stats, regret_accumulated = run_experiment(bandit, algorithms, steps, runs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
